{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER:  POS-BiLSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aWbTdkJpozX",
        "colab_type": "text"
      },
      "source": [
        "# NER  Bi-LSTM\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R1-QaP484aC",
        "colab_type": "code",
        "outputId": "354cd87e-0b9d-4cc6-fc6b-04c49139e2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_folder = '/data/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kld5BF0n6I5d",
        "colab_type": "code",
        "outputId": "83934f1e-3aaf-42af-c757-92d6ef23fb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!pip install conllu\n",
        "!pip install torchtext --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/03/4a952eb39cdc8da80a6a2416252e71784dda6bf9d726ab98065fff2aeb73/conllu-2.3.2-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-2.3.2\n",
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.5.0+cu101)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Xerxoh6I2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "from conllu import parse as conllu_parse\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NJzRmIHCIuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = os.path.join(root_folder, \"train.tsv\")\n",
        "dev_path = os.path.join(root_folder, \"dev.tsv\")\n",
        "test_path = os.path.join(root_folder, \"test.tsv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyu6VfynOo5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoaYxSpTCzgm",
        "colab_type": "text"
      },
      "source": [
        "# POS Tagging Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTlIAtitp5ti",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9QeF5eICy6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTaggingDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               input_file:str,\n",
        "               window_size:int,\n",
        "               window_shift:int=-1,\n",
        "               device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    We assume that the dataset pointed by input_file is already tokenized\n",
        "    and can fit in memory.\n",
        "    Args:\n",
        "        input_file(str): path to load the dataset\n",
        "        window_size(int): max length of a sentence in terms of number of tokens.\n",
        "        window_shift(int): The number of tokens we shift the window over the sentence.\n",
        "        Default value is -1 meaning that the window will be shifted by window_size\n",
        "        device(str): device where to put tensors(cpu or cuda).\n",
        "    \"\"\"\n",
        "\n",
        "    self.input_file = input_file\n",
        "    self.window_size = window_size\n",
        "    self.window_shift = window_shift if window_shift > 0 else window_size\n",
        "    with open(input_file) as reader:\n",
        "      # read the entire file with reader.read() and parse it\n",
        "      sentences = conllu_parse(reader.read())\n",
        "    self.device = device\n",
        "    self.data = self.create_windows(sentences)\n",
        "    self.encoded_data = None\n",
        "\n",
        "  def index_dataset(self, l_vocabulary, l_label_vocabulary):\n",
        "    self.encoded_data = list()\n",
        "    for i in range(len(self.data)):\n",
        "      # for each wondow\n",
        "      elem = self.data[i]\n",
        "      encoded_elem = torch.LongTensor(self.encode_text(elem, l_vocabulary)).to(self.device)\n",
        "      # for each element d in the elem window (d is a dict with the various fields from CoNLL line)\n",
        "      encoded_labels = torch.LongTensor([l_label_vocabulary[d[\"lemma\"]] if d is not None\n",
        "                                         else l_label_vocabulary[\"<pad>\"] for d in elem]).to(self.device)\n",
        "      self.encoded_data.append({\"inputs\":encoded_elem,\n",
        "                                \"outputs\":encoded_labels})\n",
        "    \n",
        "  def create_windows(self, sentences):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sentences(list of list of dictionary,\n",
        "                    where each dictionary represents a word occurence parsed from CoNLL line)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for sentence in sentences:\n",
        "      for d in sentence:\n",
        "        d[\"form\"] = d[\"form\"].lower() if d[\"lemma\"] == 'O' else d[\"form\"]\n",
        "      for i in range(0, len(sentence), self.window_shift):\n",
        "        window = sentence[i:i+self.window_size]\n",
        "        if len(window) < self.window_size:\n",
        "          window = window + [None]*(self.window_size - len(window))\n",
        "        assert len(window) == self.window_size\n",
        "        data.append(window)\n",
        "    return data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.encoded_data is None:\n",
        "      raise RuntimeError(\"\"\"Trying to retrieve elements but index_dataset\n",
        "            has not been invoked yet! Be sure to invoce index_dataset on this object\n",
        "            before trying to retrieve elements. In case you want to retrieve raw\n",
        "            elements, use the method get_raw_element(idx)\"\"\")\n",
        "    return self.encoded_data[idx]\n",
        "  \n",
        "  def get_raw_element(self, idx):\n",
        "    return self.data[idx]\n",
        "  \n",
        "  @staticmethod\n",
        "  def encode_text(sentence:list,\n",
        "                  l_vocabulary:Vocab):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sentences(list): list of OrderedDict, each carying the information about one token.\n",
        "        l_vocabulary(Vocab): vocabulary with mappings from words to inidices and viceversa\n",
        "    Return:\n",
        "        The method returns a list indices corresponding to the input tokens.\n",
        "    \"\"\"\n",
        "    indices = list()\n",
        "    for w in sentence:\n",
        "      if w is None:\n",
        "        indices.append(l_vocabulary[\"<pad>\"])\n",
        "      elif w[\"form\"] in l_vocabulary.stoi: # vocab str to int\n",
        "        indices.append(l_vocabulary[w[\"form\"]])\n",
        "      else:\n",
        "        indices.append(l_vocabulary.unk_index)\n",
        "    return indices\n",
        "  \n",
        "  @staticmethod\n",
        "  def decode_output(outputs:torch.Tensor,\n",
        "                    l_label_vocabulary: Vocab):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        outputs(Tensor): a Tensor woth shape(batch_size, max_len, label_vocab_size)\n",
        "          containing the logits outputed by the neural network.\n",
        "        l_label_vocabulary(Vocab): is the vocabulary containing the mapping from\n",
        "          a string label to its corresponding index and vicebversa\n",
        "        Output:\n",
        "          The method returns a list of batch_size length where each element is a list\n",
        "          of labels, one for each input token.\n",
        "    \"\"\"\n",
        "    max_indices = torch.argmax(outputs, -1).tolist() # shape = (batch_size, max_len)\n",
        "    prediction = list()\n",
        "    for indices in max_indices:\n",
        "      # Vocab integer to str is used to obtain the corresponding word from the max index\n",
        "      prediction.append([l_label_vocabulary.itos[i] for i in indices])\n",
        "    return prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS41ZdQkDggG",
        "colab_type": "code",
        "outputId": "07e1a272-e007-4539-91f6-aaac45a7f104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Create simple dataset instace for testing purpose\n",
        "\n",
        "def test_dataset_class():\n",
        "  window_size, window_shift = 30, 30\n",
        "  dataset = POSTaggingDataset(test_path, window_size, window_shift)\n",
        "\n",
        "  print('Dataset test:')\n",
        "  for i in range(10):\n",
        "    print(' sample {}: {}'.format(i, [t[\"form\"] + \":\" + t[\"lemma\"] for t in dataset.get_raw_element(i) if t is not None]))\n",
        "\n",
        "test_dataset_class()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset test:\n",
            " sample 0: ['however:O', ',:O', 'on:O', 'may:O', '8th:O', ',:O', '2010:O', ',:O', 'a:O', 'sighting:O', 'of:O', 'a:O', 'gray:O', 'whale:O', 'was:O', 'confirmed:O', 'off:O', 'the:O', 'coast:O', 'of:O', 'Israel:LOC', 'in:O', 'the:O', 'Mediterranean:LOC', 'Sea:LOC', '.:O', ',:O', 'leading:O', 'some:O', 'scientists:O']\n",
            " sample 1: ['to:O', 'think:O', 'they:O', 'might:O', 'be:O', 'repopulating:O', 'old:O', 'breeding:O', 'grounds:O', 'that:O', 'have:O', 'not:O', 'been:O', 'used:O', 'for:O', 'centuries:O', '.:O']\n",
            " sample 2: ['the:O', 'plot:O', 'focuses:O', 'on:O', 'a:O', 'brutal:O', 'and:O', 'cunning:O', 'group:O', 'of:O', 'recently:O', 'escaped:O', 'replicants:O', 'hiding:O', 'in:O', 'L.A.:LOC', 'and:O', 'the:O', 'semi-retired:O', 'blade:O', 'runner:O', ',:O', 'rick:O', 'deckard:O', ',:O', 'who:O', 'reluctantly:O', 'agrees:O', 'to:O', 'take:O']\n",
            " sample 3: ['on:O', 'one:O', 'more:O', 'assignment:O', 'to:O', 'hunt:O', 'them:O', 'all:O', 'down:O', ',:O', 'while:O', 'searching:O', 'for:O', 'his:O', 'own:O', 'identity:O', '.:O']\n",
            " sample 4: ['during:O', 'his:O', 'old:O', 'age:O', ',:O', 'David:PER', 'spends:O', 'his:O', 'nights:O', 'with:O', 'Abishag:PER', ',:O', 'a:O', 'woman:O', 'appointed:O', 'for:O', 'the:O', 'purpose:O', 'of:O', 'keeping:O', 'him:O', 'warm:O', '.:O']\n",
            " sample 5: ['large:O', 'national:O', 'department:O', 'stores:O', 'also:O', 'arrived:O', 'downtown:O', ',:O', 'including:O', 'Montgomery:ORG', 'Ward:ORG', ',:O', 'Sears:ORG', ',:O', 'and:O', 'J.C.:ORG', 'Penney:ORG', '.:O']\n",
            " sample 6: ['he:O', 'ordered:O', 'all:O', 'bridges:O', 'across:O', 'the:O', 'Tiber:LOC', 'cut:O', ',:O', 'reportedly:O', 'on:O', 'the:O', 'counsel:O', 'of:O', 'the:O', 'gods:O', ',:O', 'and:O', 'left:O', 'the:O', 'rest:O', 'of:O', 'central:O', 'Italy:LOC', 'undefended:O', ';:O', 'Constantine:PER', 'secured:O', 'that:O', 'region:O']\n",
            " sample 7: [\"'s:O\", 'support:O', 'without:O', 'challenge:O', '.:O']\n",
            " sample 8: ['having:O', 'spent:O', 'the:O', 'greater:O', 'part:O', 'of:O', 'his:O', 'early:O', 'life:O', 'in:O', 'the:O', 'twilight:O', 'of:O', 'Nero:PER', \"'s:O\", 'reign:O', ',:O', 'his:O', 'formative:O', 'years:O', 'would:O', 'have:O', 'been:O', 'strongly:O', 'influenced:O', 'by:O', 'the:O', 'political:O', 'turmoil:O', 'of:O']\n",
            " sample 9: ['the:O', '60s:O', ',:O', 'culminating:O', 'with:O', 'the:O', 'civil:O', 'war:O', 'of:O', '69:O', ',:O', 'which:O', 'brought:O', 'his:O', 'family:O', 'to:O', 'power:O', '.:O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBZ4DbtagNN",
        "colab_type": "text"
      },
      "source": [
        "Build Vocab and label Vocab using TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MR24KbPlwAH",
        "colab_type": "code",
        "outputId": "c1f8f470-3cae-47ba-a0bd-e5d71ef8ed9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def build_vocab (dataset, min_freq=1):\n",
        "  counter = Counter()\n",
        "  for i in tqdm(range(len(dataset))):\n",
        "    # for each token in the sentence viewed as a dictionary of items for the CoNLL line\n",
        "    for token in dataset.get_raw_element(i):\n",
        "      if token is not None:\n",
        "        counter[token[\"form\"]]+=1\n",
        "  # we add special tokens for handling padding and unk words at testing time\n",
        "  return Vocab(counter, specials=['<pad>', '<unk>'], min_freq=min_freq)\n",
        "\n",
        "def build_label_vocab(dataset):\n",
        "  counter = Counter()\n",
        "  for i in tqdm(range(len(dataset))):\n",
        "    for token in dataset.get_raw_element(i):\n",
        "      if token is not None:\n",
        "        counter[token[\"lemma\"]]+=1\n",
        "  # no unk token for label\n",
        "  return Vocab(counter, specials=['<pad>'])\n",
        "  \n",
        "window_size, window_shift = 100,100\n",
        "dataset = POSTaggingDataset(train_path, window_size,window_shift)\n",
        "vocabulary = build_vocab(dataset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(dataset)\n",
        "dataset.index_dataset(vocabulary, label_vocabulary)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100042/100042 [00:01<00:00, 50373.69it/s]\n",
            "100%|██████████| 100042/100042 [00:01<00:00, 84269.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2bY4V1sLPV",
        "colab_type": "text"
      },
      "source": [
        "# Model Defenition\n",
        "## Embeeding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGGpNrRlsaY3",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pmAFtNgsY4R",
        "colab_type": "code",
        "outputId": "b805c362-5f8a-4b64-9258-e16c7ab8c196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lstm =nn.LSTM(10,5)\n",
        "lstm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(10, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yNjN9zhsphH",
        "colab_type": "text"
      },
      "source": [
        "# Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuIDA8W6sn-M",
        "colab_type": "code",
        "outputId": "605e194a-7b34-4201-d074-09a5e47348d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classifier = nn.Linear(5, len(label_vocabulary))\n",
        "classifier"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=5, out_features=5, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lce9P_fqs5oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSBiLstmModel(nn.Module):\n",
        "  # we provide hyperparameters as input\n",
        "  def __init__(self, hparams):\n",
        "    super(POSBiLstmModel, self).__init__()\n",
        "    # Embedding layer: a matrix vocab size x_embedding_dim where each index\n",
        "    # correspond to a word in the vocabulary and the i-th row corresponds to \n",
        "    # a latent representation of the i-th word in the vocab.\n",
        "    pprint(params)\n",
        "    self.word_embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n",
        "    if hparams.embeddings is not None:\n",
        "      print(\"initializing embeddings from pretrained\")\n",
        "      self.word_embedding.weight.data.copy_(hparams.embeddings)\n",
        "\n",
        "    # LSTM layer: an LSTM nn that process the input text\n",
        "    # (encoded wiith word embeddings) from left to right as outputs\n",
        "    # a new **contextual** representation of each word that depend\n",
        "    # on the preciding words.\n",
        "    self.lstm = nn.LSTM(hparams.embedding_dim, hparams.hidden_dim,\n",
        "                        bidirectional = hparams.bidirectional,\n",
        "                        num_layers = hparams.num_layers,\n",
        "                        dropout = hparams.dropout if hparams.num_layers > 1 else 0)\n",
        "    # Hidden layer: transforms the input value/ scalar into\n",
        "    # a hidden vector representation\n",
        "    lstm_output_dim = hparams.hidden_dim if hparams.bidirectional is False else hparams.hidden_dim * 2\n",
        "\n",
        "    # During training, randomly zeroes some of the elements of the input tensor\n",
        "    # with probability hparams.dropouts using samples from a Bernoulli\n",
        "    # distribution. Each channel will be zeroed out independently on every fwd call.\n",
        "    # This has proven to be an effective technique for regularization and preventing\n",
        "    # the co-adaption of neurons\n",
        "    self.dropout = nn.Dropout(hparams.dropout)\n",
        "    self.classifier = nn.Linear(lstm_output_dim, hparams.num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeddings = self.word_embedding(x)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    o, (h, c) = self.lstm(embeddings)\n",
        "    o = self.dropout(o)\n",
        "    output = self.classifier(o)\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDZa07z2tCAE",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_oJxNpJtAZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HParams():\n",
        "  vocab_size = len(vocabulary)\n",
        "  hidden_dim = 128\n",
        "  embedding_dim = 100\n",
        "  num_classes = len(label_vocabulary) # num of different  universal POS tagging\n",
        "  bidirectional = True\n",
        "  num_layers = 1\n",
        "  dropout = 0.0\n",
        "  embeddings = None\n",
        "params = HParams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb9bTapztIBf",
        "colab_type": "code",
        "outputId": "161a3c4d-9ce7-4e62-c0f2-b8035e3fe049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "PosBilstm = POSBiLstmModel(params).cuda()\n",
        "PosBilstm"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.HParams object at 0x7f64dae74fd0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "POSBiLstmModel(\n",
              "  (word_embedding): Embedding(50201, 100)\n",
              "  (lstm): LSTM(100, 128, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dCtpSktKVD",
        "colab_type": "code",
        "outputId": "19f59339-1190-4e1a-ae95-944af97c78be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "source": [
        "encoded_input = [x[\"inputs\"] for x in dataset[:10]]\n",
        "logits = PosBilstm(torch.stack(encoded_input, 0).cuda())\n",
        "labels = POSTaggingDataset.decode_output(logits, label_vocabulary)\n",
        "for i in range(2):\n",
        "  i_sentence = dataset.get_raw_element(i)\n",
        "  i_labels = labels[i]\n",
        "  pprint(list(zip([w[\"form\"] for w in i_sentence if w is not None], i_labels)))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Burgdorf', 'ORG'),\n",
            " ('had', 'O'),\n",
            " ('brought', 'ORG'),\n",
            " ('a', 'PER'),\n",
            " ('capsule', 'ORG'),\n",
            " ('of', 'PER'),\n",
            " ('cyanide', '<pad>'),\n",
            " ('for', 'LOC'),\n",
            " ('the', 'ORG'),\n",
            " ('occasion', '<pad>'),\n",
            " ('.', 'PER')]\n",
            "[('Terry', 'ORG'),\n",
            " ('Daniher', 'PER'),\n",
            " ('and', 'ORG'),\n",
            " ('his', 'O'),\n",
            " ('brother', 'PER'),\n",
            " ('Neale', 'PER'),\n",
            " ('would', 'O'),\n",
            " ('come', 'LOC'),\n",
            " ('via', 'LOC'),\n",
            " ('a', 'O'),\n",
            " ('trade', 'PER'),\n",
            " ('with', 'O'),\n",
            " ('South', 'O'),\n",
            " ('Melbourne', 'O'),\n",
            " (',', 'PER'),\n",
            " ('and', 'O'),\n",
            " ('Roger', 'PER'),\n",
            " ('Merrett', 'PER'),\n",
            " ('joined', 'PER'),\n",
            " ('soon', 'PER'),\n",
            " ('afterwards', 'LOC'),\n",
            " ('to', '<pad>'),\n",
            " ('form', 'LOC'),\n",
            " ('the', 'O'),\n",
            " ('nucleus', 'O'),\n",
            " ('of', 'PER'),\n",
            " ('what', 'PER'),\n",
            " ('would', '<pad>'),\n",
            " ('become', 'PER'),\n",
            " ('the', 'ORG'),\n",
            " ('formidable', 'LOC'),\n",
            " ('Essendon', 'LOC'),\n",
            " ('sides', '<pad>'),\n",
            " ('of', 'LOC'),\n",
            " ('the', 'ORG'),\n",
            " ('1980s', 'LOC'),\n",
            " ('.', 'PER')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgTGrBYtSBx",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azX6m7sJtPBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "  \"\"\"utility class to train and evaluate a model.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               model: nn.Module,\n",
        "               loss_function,\n",
        "               optimizer,\n",
        "               label_vocab: Vocab,\n",
        "               log_steps: int=10_000,\n",
        "               log_level:int=2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model: medel we want to train.\n",
        "        loss_function: the loss_function to be minimized.\n",
        "        optimizer: to minimize loss function.\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.loss_function = loss_function\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.label_vocab = label_vocab\n",
        "    self.log_steps = log_steps\n",
        "    self.log_level = log_level\n",
        "  \n",
        "  def train(self, train_dataset:Dataset,\n",
        "            valid_dataset:Dataset,\n",
        "            epochs:int=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_dataset: a Dataset or DataLoader instance containing the training instance.\n",
        "        valid_dataset: used to evaluate learning progress.\n",
        "        ephocs: the number of times to iterate over train_dataset.\n",
        "\n",
        "    Returns:\n",
        "        avg_train_loss: avg training loss on train_dataset over epochs.\n",
        "    \"\"\"\n",
        "    assert epochs > 1 and isinstance(epochs, int)\n",
        "    if self.log_level > 0:\n",
        "      print(\"Training...\")\n",
        "    train_loss = 0.0\n",
        "    for epoch in range(epochs):\n",
        "      if self.log_level > 0:\n",
        "        print(\"Epoch {:03d}\".format(epoch + 1))\n",
        "\n",
        "      epoch_loss = 0.0\n",
        "      self.model.train()\n",
        "\n",
        "      for step, sample in enumerate(train_dataset):\n",
        "        inputs = sample['inputs']\n",
        "        labels = sample['outputs']\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        predictions = self.model(inputs)\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        sample_loss = self.loss_function(predictions, labels)\n",
        "        sample_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        epoch_loss += sample_loss.tolist()\n",
        "\n",
        "        if self.log_level > 1 and step % self.log_steps == self.log_steps -1:\n",
        "          print('\\t[E: {:2d} 0 step{}] curret avg_loss = {:0.4f}'.format(epoch, step, epoch_loss / (step + 1)))\n",
        "      \n",
        "      avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "      train_loss += avg_epoch_loss\n",
        "      if self.log_level > 0:\n",
        "        print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "      valid_loss = self.evaluate(valid_dataset)\n",
        "      if self.log_level > 0:\n",
        "        print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
        "      \n",
        "    if self.log_level > 0:\n",
        "      print('..... Done!')\n",
        "\n",
        "    avg_epoch_loss = train_loss / epochs\n",
        "    return avg_epoch_loss\n",
        "  \n",
        "  def evaluate(self, valid_dataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        valid_dataset: dataset for evaluation.\n",
        "    Returns:\n",
        "        avg_valid_loss: the average validation loss over valid_dataset.\n",
        "    \"\"\"\n",
        "    valid_loss = 0.0\n",
        "    # set the dropout to 0!! Nedded when we are in inference mode.\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      for sample in valid_dataset:\n",
        "        inputs = sample['inputs']\n",
        "        labels = sample['outputs']\n",
        "\n",
        "        predictions = self.model(inputs)\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        labels = labels.view(-1)\n",
        "        sample_loss = self.loss_function(predictions, labels)\n",
        "        valid_loss += sample_loss.tolist()\n",
        "    return valid_loss / len(valid_dataset)\n",
        "  \n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "        x: a tensor of indices.\n",
        "    Returns:\n",
        "        A list containing the predicted POS tag for each token in input sentences\n",
        "    \"\"\"\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits = self.model(x)\n",
        "      predictions = torch.argmax(logits, -1)\n",
        "      return logits, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grW7tQ2kte8z",
        "colab_type": "text"
      },
      "source": [
        "Define dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzhV09MZtYI2",
        "colab_type": "code",
        "outputId": "4dce89b9-f61a-4d54-912d-aa6eb7fd1e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "window_size, window_shift = 100, 100\n",
        "device = \"cuda\"\n",
        "trainingset = POSTaggingDataset(train_path, window_size, window_shift, device=device)\n",
        "devset = POSTaggingDataset(dev_path, window_size, window_shift, device=device)\n",
        "testset = POSTaggingDataset(test_path, window_size, window_shift, device=device)\n",
        "\n",
        "trainingset.index_dataset(vocabulary, label_vocabulary)\n",
        "devset.index_dataset(vocabulary, label_vocabulary)\n",
        "testset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "train_dataset = DataLoader(trainingset, batch_size = 128)\n",
        "valid_dataset = DataLoader(devset, batch_size = 128)\n",
        "test_dataset = DataLoader(testset, batch_size = 128)\n",
        "\n",
        "PosBilstm = POSBiLstmModel(params).cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.HParams object at 0x7f64dae74fd0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1p1yuCuMMX",
        "colab_type": "text"
      },
      "source": [
        "set up a trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwSwZU5guOPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = PosBilstm,\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=label_vocabulary['<pad>']),\n",
        "    optimizer = optim.Adam(PosBilstm.parameters()),\n",
        "    label_vocab=label_vocabulary\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sRYwCrsut1B",
        "colab_type": "code",
        "outputId": "822e6b0f-e454-4bed-ffdc-c4ea708cc80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "trainer.train(train_dataset, valid_dataset, 10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch 001\n",
            "\t[E:  0] train loss = 0.2824\n",
            "  [E:  0] valid loss = 0.1718\n",
            "Epoch 002\n",
            "\t[E:  1] train loss = 0.1392\n",
            "  [E:  1] valid loss = 0.1224\n",
            "Epoch 003\n",
            "\t[E:  2] train loss = 0.1040\n",
            "  [E:  2] valid loss = 0.1049\n",
            "Epoch 004\n",
            "\t[E:  3] train loss = 0.0878\n",
            "  [E:  3] valid loss = 0.0975\n",
            "Epoch 005\n",
            "\t[E:  4] train loss = 0.0790\n",
            "  [E:  4] valid loss = 0.0943\n",
            "Epoch 006\n",
            "\t[E:  5] train loss = 0.0735\n",
            "  [E:  5] valid loss = 0.0935\n",
            "Epoch 007\n",
            "\t[E:  6] train loss = 0.0693\n",
            "  [E:  6] valid loss = 0.0942\n",
            "Epoch 008\n",
            "\t[E:  7] train loss = 0.0652\n",
            "  [E:  7] valid loss = 0.0962\n",
            "Epoch 009\n",
            "\t[E:  8] train loss = 0.0608\n",
            "  [E:  8] valid loss = 0.0993\n",
            "Epoch 010\n",
            "\t[E:  9] train loss = 0.0559\n",
            "  [E:  9] valid loss = 0.1037\n",
            "..... Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10171731371108603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYVf6IlFuw1L",
        "colab_type": "code",
        "outputId": "669880c2-20c8-4ed5-ed26-d3226e4f3c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_set_loss = trainer.evaluate(test_dataset)\n",
        "print(\"test set loss: {}\".format(test_set_loss))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set loss: 0.1020301320582382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGcljN8PvPsJ",
        "colab_type": "code",
        "outputId": "ecbf238b-3806-444c-fdf2-3b1fd39c6d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def print_outputs(l_trainer, l_testset, num_outputs, l_vocabulary, l_label_vocabulary):\n",
        "\n",
        "  for i in range(num_outputs):\n",
        "    print(\"sentence {}\".format(i))\n",
        "    print()\n",
        "    test_elem = l_testset[i]\n",
        "\n",
        "    test_x, test_y = test_elem[\"inputs\"], test_elem[\"outputs\"]\n",
        "\n",
        "    logits, predictions = l_trainer.predict(test_x.unsqueeze(0))\n",
        "\n",
        "    decoded_labels = POSTaggingDataset.decode_output(logits, l_label_vocabulary)[0]\n",
        "    test_y = test_y.tolist()\n",
        "    print(\"token\\t\\tinput\\t\\tgold\\t\\tprediction\")\n",
        "    print(\"-\"*100)\n",
        "    for raw_elem, idx, label, predicted_label in zip(l_testset.get_raw_element(i), test_x.tolist(), test_y, decoded_labels):\n",
        "      if idx == 0:\n",
        "        break\n",
        "      print(\"{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(raw_elem[\"form\"], l_vocabulary.itos[idx], l_label_vocabulary.itos[label], predicted_label))\n",
        "    print(\"=\"*30)\n",
        "\n",
        "print_outputs(trainer, testset, 3, vocabulary, label_vocabulary)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence 0\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "however\t\thowever\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "on\t\ton\t\tO\t\tO\n",
            "may\t\tmay\t\tO\t\tO\n",
            "8th\t\t8th\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "2010\t\t2010\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "sighting\t\tsighting\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "gray\t\tgray\t\tO\t\tO\n",
            "whale\t\twhale\t\tO\t\tO\n",
            "was\t\twas\t\tO\t\tO\n",
            "confirmed\t\tconfirmed\t\tO\t\tO\n",
            "off\t\toff\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "coast\t\tcoast\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "Israel\t\tIsrael\t\tLOC\t\tLOC\n",
            "in\t\tin\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "Mediterranean\t\tMediterranean\t\tLOC\t\tLOC\n",
            "Sea\t\tSea\t\tLOC\t\tLOC\n",
            ".\t\t.\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "leading\t\tleading\t\tO\t\tO\n",
            "some\t\tsome\t\tO\t\tO\n",
            "scientists\t\tscientists\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "think\t\tthink\t\tO\t\tO\n",
            "they\t\tthey\t\tO\t\tO\n",
            "might\t\tmight\t\tO\t\tO\n",
            "be\t\tbe\t\tO\t\tO\n",
            "repopulating\t\t<unk>\t\tO\t\tO\n",
            "old\t\told\t\tO\t\tO\n",
            "breeding\t\tbreeding\t\tO\t\tO\n",
            "grounds\t\tgrounds\t\tO\t\tO\n",
            "that\t\tthat\t\tO\t\tO\n",
            "have\t\thave\t\tO\t\tO\n",
            "not\t\tnot\t\tO\t\tO\n",
            "been\t\tbeen\t\tO\t\tO\n",
            "used\t\tused\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "centuries\t\tcenturies\t\tO\t\tO\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n",
            "sentence 1\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "the\t\tthe\t\tO\t\tO\n",
            "plot\t\tplot\t\tO\t\tO\n",
            "focuses\t\tfocuses\t\tO\t\tO\n",
            "on\t\ton\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "brutal\t\tbrutal\t\tO\t\tO\n",
            "and\t\tand\t\tO\t\tO\n",
            "cunning\t\tcunning\t\tO\t\tO\n",
            "group\t\tgroup\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "recently\t\trecently\t\tO\t\tO\n",
            "escaped\t\tescaped\t\tO\t\tO\n",
            "replicants\t\treplicants\t\tO\t\tO\n",
            "hiding\t\thiding\t\tO\t\tO\n",
            "in\t\tin\t\tO\t\tO\n",
            "L.A.\t\tL.A.\t\tLOC\t\tLOC\n",
            "and\t\tand\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "semi-retired\t\t<unk>\t\tO\t\tO\n",
            "blade\t\tblade\t\tO\t\tO\n",
            "runner\t\trunner\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "rick\t\t<unk>\t\tO\t\tO\n",
            "deckard\t\tdeckard\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "who\t\twho\t\tO\t\tO\n",
            "reluctantly\t\treluctantly\t\tO\t\tO\n",
            "agrees\t\tagrees\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "take\t\ttake\t\tO\t\tO\n",
            "on\t\ton\t\tO\t\tO\n",
            "one\t\tone\t\tO\t\tO\n",
            "more\t\tmore\t\tO\t\tO\n",
            "assignment\t\tassignment\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "hunt\t\thunt\t\tO\t\tO\n",
            "them\t\tthem\t\tO\t\tO\n",
            "all\t\tall\t\tO\t\tO\n",
            "down\t\tdown\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "while\t\twhile\t\tO\t\tO\n",
            "searching\t\tsearching\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "own\t\town\t\tO\t\tO\n",
            "identity\t\tidentity\t\tO\t\tO\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n",
            "sentence 2\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "during\t\tduring\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "old\t\told\t\tO\t\tO\n",
            "age\t\tage\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "David\t\tDavid\t\tPER\t\tPER\n",
            "spends\t\tspends\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "nights\t\tnights\t\tO\t\tO\n",
            "with\t\twith\t\tO\t\tO\n",
            "Abishag\t\t<unk>\t\tPER\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "woman\t\twoman\t\tO\t\tO\n",
            "appointed\t\tappointed\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "purpose\t\tpurpose\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "keeping\t\tkeeping\t\tO\t\tO\n",
            "him\t\thim\t\tO\t\tO\n",
            "warm\t\twarm\t\tO\t\tO\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkvCDNh9vTou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score as sk_precision\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "def evaluation(model:nn.Module, l_dataset:DataLoader, l_label_vocab:Vocab):\n",
        "  all_predictions = list()\n",
        "  all_labels = list()\n",
        "  for indexed_elem in l_dataset:\n",
        "    indexed_in = indexed_elem[\"inputs\"]\n",
        "    indexed_labels = indexed_elem[\"outputs\"]\n",
        "    predictions = model(indexed_in)\n",
        "    predictions = torch.argmax(predictions, -1).view(-1)\n",
        "    labels = indexed_labels.view(-1)\n",
        "    valid_indices = labels != 0\n",
        "\n",
        "    valid_predictions = predictions[valid_indices]\n",
        "    valid_labels = labels[valid_indices]\n",
        "\n",
        "    all_predictions.extend(valid_predictions.tolist())\n",
        "    all_labels.extend(valid_labels.tolist())\n",
        "  precision = sk_precision(all_labels, all_predictions, average=\"macro\")\n",
        "  recall = recall_score(all_labels, all_predictions, average=\"macro\")\n",
        "  f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
        "  return{\"precision\":precision,\n",
        "         \"recall\":recall,\n",
        "         \"f1\":f1\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eusaFEQhupe6",
        "colab_type": "code",
        "outputId": "9a8090dc-12a2-42bb-d24b-00bc3e9034a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate = evaluation(PosBilstm, test_dataset, label_vocabulary)\n",
        "print(evaluate)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'precision': 0.8675922382923663, 'recall': 0.785131802067354, 'f1': 0.8224439846909528}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}